Because training and testing images are stored there, I mounted Google Drive on Colab to access these images smoothly. I then loaded the provided train.csv, which matches each image's filename ( ImageId) to a defect class (ClassId). Since some images appeared more than once, one for each defect annotation, I collapsed those duplicates by grouping on ImageId to select the most frequent class label so each image has only one valid ground-truth label. The cleaned labels were then split 80/20 into training and validation, stratified by defect class to maintain a balanced distribution.
Next up, I created a custom PyTorch Dataset that fetches each image file via its "bare" ID, applies a series of transforms (random horizontal flips, resizing 224×224, and normalization to ImageNet stats during training; resizing and normalization in validation), and spits out the image tensor along with the label. For the model, I employed a pretrained ResNet-18 backbone from torchvision. models, swapping its last fully connected layer for a linear classifier with four ways corresponding to predicting the four types of defects. Training the network was done with cross-entropy loss and the Adam optimizer (lr= 1×10⁻⁴) for five epochs, saving the checkpoint model to Drive whenever the validation accuracy was improved.
For inference, I constructed yet another Dataset/DataLoader over the test_images folder, mapping bare IDs to their respective filenames, and subsequently loaded the best checkpoint in evaluation mode. I then poured the test images into the network, took argmax over outputs (shifting the zero-based indices back to 1–4), and recorded the results into a submission.csv file with columns ImageId and ClassId.
Reproducing the whole pipeline involves opening the Colab notebook, mounting the Drive, and setting the BASE_DIR variable to the folder that contains your data, running the cells in order: install dependencies, label cleaning, define datasets and loaders, training and checkpointing of the model, and finally inference for the generation of the submission file. After five epochs, the validation accuracy that I achieved was roughly XX percent; more training with learning-rate scheduling, enhanced data augmentations (rotations, color jitter), attempting more powerful backbone architectures, or ensembling a couple of models would be great next steps. This project was developed for an interview exercise to demonstrate full deep learning workflow capabilities from data preparation and augmentation to model training and checkpointing, ending in final inference, so I am ready to discuss any portion of it and delve into possible improvements.
